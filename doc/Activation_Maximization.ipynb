{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activation Maximization**\n",
    "#### Overview\n",
    "In this project, we use the most straight-forward technique: Activation Maximization to optimize the feature visualization.\n",
    "\n",
    "Activation Maximization is proposed to visualize the preferred inputs of neurons in each layer. The preferred input can indicate what features of a neurons has learned.[[1]](https://arxiv.org/pdf/1804.11191.pdf) The learned feature is represented by a synthesized input pattern that can give rise to the highest activation of a target neuron. In order to find such input pattern, each pixel of the CNN's input is iteratively changed to get the highest activation of the target neuron. \n",
    "\n",
    "#### Activation Maximization algorithm \n",
    "\n",
    "We may pose the activation maximization problem for a unit with index j on a layer l of a network $\\Phi$ as finding an image x* where:\n",
    "$$x^* = arg_{x}max\\phi_{l,j}(\\theta,x)$$ and $\\theta$ denotes the network parameters sets (weight and bias).[[2]](https://arxiv.org/pdf/1602.03616.pdf)\n",
    "There are four main steps in this process:[[3]](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
    "\n",
    "(1)Build a loss function that maximizes the activation of the particular filter.\n",
    "\n",
    "(2)Compute the gradient of the input picture with respect to this loss function and move x in the direction of this gradient.\n",
    "\n",
    "(3)Normalize trick: we normalize the gradient of the pixels of the input image(which avoids very small and very large gradients and ensures a smooth gradient ascent process).\n",
    "\n",
    "(4)Build a iteration function that returns the loss and gradients given the input image.\n",
    "We start from a noise image, then iterate the interation function for enough time. The process terminates at a certain image x* when the image without any noise.\n",
    "\n",
    "In order to overcome the uninterpretability problem of Acitivation Maximization in the Deep Neural Network, regularization methods have been implemented to collectively improve the image quality. AM with regulation:\n",
    "$$x^* = arg_{x}max(\\phi_{l,j}(\\theta,x)-\\lambda(x))$$ where $\\lambda(x)$ is a parameterized regularization function.[[1]](https://arxiv.org/pdf/1804.11191.pdf)\n",
    "\n",
    "In the following sections, we utilize the function of *visualize_activation* from Keras-vis package to achieve Activation Maximization. The function is referenced in (https://raghakot.github.io/keras-vis/visualizations/activation_maximization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
